{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sklearn\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df2=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df3=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df4=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df5=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
    "df6=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df7=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df8=pd.read_csv(\"D:/Users/Desktop/Scientific-research/Reinforcement Learning/Env/CICIDS2017/MachineLearningCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "\n",
    "df = pd.concat([df1,df2])\n",
    "df = pd.concat([df,df3])\n",
    "df = pd.concat([df,df4])\n",
    "df = pd.concat([df,df5])\n",
    "df = pd.concat([df,df6])\n",
    "df = pd.concat([df,df7])\n",
    "df = pd.concat([df,df8])\n",
    "\n",
    "del df1,df2,df3,df4,df5,df6,df7,df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total missing values: \", sum(list(df.isna().sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total missing values: \", sum(list(df.isna().sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[' Label'].values\n",
    "\n",
    "X=df.drop([' Label'], axis=1).values\n",
    "\n",
    "print(y.reshape(-1, 1).shape)\n",
    "print('================')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sizes = []\n",
    "labels = []\n",
    "\n",
    "for i in Counter(y):\n",
    "    tmp = str(i) + ' - ' + str(Counter(y)[i])\n",
    "    labels.append(tmp)\n",
    "    sizes.append(Counter(y)[i])\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(13, 8))\n",
    "plt.plot(labels, sizes, marker='o')\n",
    "plt.title('Biểu đồ đường')\n",
    "plt.xlabel('Nhãn')\n",
    "plt.ylabel('Số lượng')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))\n",
    "\n",
    "labels=np.unique(y)\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "print('-------------------')\n",
    "\n",
    "classes = []\n",
    "for i in range(0,len(labels)):\n",
    "    classes.append(i)\n",
    "print(classes)\n",
    "print('-------------------')\n",
    "\n",
    "map=dict(zip(labels,classes))\n",
    "print(map)\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "y_class=[]\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "    y_class.append(map[y[i]])\n",
    "    \n",
    "y_class=np.asarray(y_class)\n",
    "\n",
    "print(y_class)\n",
    "print(len(y_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_correctly(state):\n",
    "#     # Thực hiện kiểm tra detect đúng hay sai cho trạng thái state\n",
    "#     # Trả về True nếu detect đúng và False nếu detect sai\n",
    "#     # Thay thế phần mã logic này bằng hàm kiểm tra detect của bạn\n",
    "    \n",
    "#     # Lấy nhãn detect tại trạng thái state\n",
    "#     detect_label = y[state]\n",
    "    \n",
    "#     # So sánh detect_label với phần tử được gán nhãn tại vị trí state\n",
    "#     if detect_label == state:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Số lượng trạng thái (states)\n",
    "# num_states = len(df)\n",
    "\n",
    "# # Tạo ánh xạ từ nhãn sang lớp\n",
    "# labels = np.unique(y)\n",
    "# classes = np.arange(len(labels))\n",
    "# label_map = dict(zip(labels, classes))\n",
    "\n",
    "# # Số lượng hành động (actions)\n",
    "# num_actions = len(label_map)\n",
    "\n",
    "# # Khởi tạo bảng Q với giá trị ban đầu là 0\n",
    "# Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# # Hệ số học (learning rate)\n",
    "# alpha = 0.1\n",
    "\n",
    "# # Hệ số giảm epsilon (exploration rate)\n",
    "# epsilon = 0.1\n",
    "\n",
    "# # Số lượng episodes\n",
    "# num_episodes = 10\n",
    "\n",
    "# # Vòng lặp qua các episodes\n",
    "# for episode in range(num_episodes):\n",
    "#     # Khởi tạo trạng thái ban đầu\n",
    "#     state = 0\n",
    "#     total_reward = 0\n",
    "#     done = False\n",
    "    \n",
    "#     # Lặp qua các bước trong episode\n",
    "#     while not done:\n",
    "#         # Chọn hành động dựa trên chiến lược epsilon-greedy\n",
    "#         if np.random.uniform() < epsilon:\n",
    "#             # Hành động ngẫu nhiên\n",
    "#             action = np.random.randint(num_actions)\n",
    "#         else:\n",
    "#             # Hành động tốt nhất dựa trên bảng Q\n",
    "#             action = np.argmax(Q[state])\n",
    "        \n",
    "#         # Thực hiện hành động và nhận được phần thưởng (reward)\n",
    "#         next_state = state + 1  # Thay thế bằng hành động thực tế của bạn\n",
    "        \n",
    "#         # Kiểm tra xem next_state có nằm trong phạm vi hợp lệ hay không\n",
    "#         if next_state >= num_states:\n",
    "#             done = True  # Đánh dấu là đã hoàn thành episode\n",
    "#             break\n",
    "        \n",
    "#         # Kiểm tra xem detect có đúng hay không\n",
    "#         if detect_correctly(next_state):  # Thay thế bằng hàm kiểm tra detect của bạn\n",
    "#             reward = 10\n",
    "#         else:\n",
    "#             reward = -1\n",
    "        \n",
    "#         # Cập nhật bảng Q\n",
    "#         Q[state, action] += alpha * (reward + np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "#         # Cộng dồn phần thưởng vào tổng reward\n",
    "#         total_reward += reward\n",
    "        \n",
    "#         # Chuyển sang trạng thái kế tiếp\n",
    "#         state = next_state\n",
    "        \n",
    "#     # In ra số episode và tổng reward sau khi hoàn thành mỗi episode\n",
    "#     print(\"Episode\", episode+1, \": total reward ->\", total_reward)\n",
    "\n",
    "# # Lựa chọn hành động tốt nhất dựa trên bảng Q\n",
    "# best_actions = np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylabel('Total Reward')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.plot([rewards[e] for e in rewards.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
