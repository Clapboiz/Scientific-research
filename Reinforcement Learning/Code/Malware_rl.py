# import numpy as np

# # Khởi tạo ma trận phần thưởng R
# R = np.array([
#     [-1, -1, -1, -1, 0, -1],
#     [-1, -1, -1, 0, -1, 100],
#     [-1, -1, -1, 0, -1, -1],
#     [-1, 0, 0, -1, 0, -1],
#     [0, -1, -1, 0, -1, 100],
#     [-1, 0, -1, -1, 0, 100]
# ])

# # Khởi tạo ma trận Q ban đầu
# Q = np.zeros_like(R)

# # Số lần lặp (epochs) và hệ số học (learning rate)
# epochs = 1000
# learning_rate = 0.8

# # Hàm Q-learning
# for _ in range(epochs):
#     # Chọn một trạng thái ngẫu nhiên
#     state = np.random.randint(0, 6)

#     # Lấy các hành động khả dụng từ trạng thái hiện tại
#     available_actions = np.where(R[state] >= 0)[0]

#     # Chọn một hành động ngẫu nhiên từ các hành động khả dụng
#     action = np.random.choice(available_actions)

#     # Tính giá trị Q mới dựa trên công thức Q-learning
#     Q[state, action] = (1 - learning_rate) * Q[state, action] + learning_rate * (R[state, action] + np.max(Q[action]))

# # In ma trận Q sau khi hoàn thành Q-learning
# print("Ma trận Q:")
# print(Q)

# # Tìm đường đi tốt nhất từ trạng thái ban đầu đến trạng thái đích
# current_state = 2  # Trạng thái ban đầu
# goal_state = 5  # Trạng thái đích
# path = [current_state]

# while current_state != goal_state:
#     next_state = np.argmax(Q[current_state])
#     path.append(next_state)
#     current_state = next_state

# # In đường đi tìm được
# print("Đường đi từ trạng thái ban đầu đến trạng thái đích:")
# print(path)


import pandas as pd


class Qlearning:
    _qmatrix = None
    _learn_rate = None
    _discount_factor = None

    def __init__(self,
                 possible_states,
                 possible_actions,
                 initial_reward,
                 learning_rate,
                 discount_factor):
        """
        Initialise the q learning class with an initial matrix and the parameters for learning.

        :param possible_states: list of states the agent can be in
        :param possible_actions: list of actions the agent can perform
        :param initial_reward: the initial Q-values to be used in the matrix
        :param learning_rate: the learning rate used for Q-learning
        :param discount_factor: the discount factor used for Q-learning
        """
        # Initialize the matrix with Q-values
        init_data = [[float(initial_reward) for _ in possible_states]
                     for _ in possible_actions]
        self._qmatrix = pd.DataFrame(data=init_data,
                                     index=possible_actions,
                                     columns=possible_states)

        # Save the parameters
        self._learn_rate = learning_rate
        self._discount_factor = discount_factor

    def get_best_action(self, state):
        """
        Retrieve the action resulting in the highest Q-value for a given state.

        :param state: the state for which to determine the best action
        :return: the best action from the given state
        """
        # Return the action (index) with maximum Q-value
        return self._qmatrix[[state]].idxmax().iloc[0]

    def update_model(self, state, action, reward, next_state):
        """
        Update the Q-values for a given observation.

        :param state: The state the observation started in
        :param action: The action taken from that state
        :param reward: The reward retrieved from taking action from state
        :param next_state: The resulting next state of taking action from state
        """
        # Update q_value for a state-action pair Q(s,a):
        # Q(s,a) = Q(s,a) + α( r + γmaxa' Q(s',a') - Q(s,a) )
        q_sa = self._qmatrix.ix[action, state]
        max_q_sa_next = self._qmatrix.ix[self.get_best_action(next_state), next_state]
        r = reward
        alpha = self._learn_rate
        gamma = self._discount_factor

        # Do the computation
        new_q_sa = q_sa + alpha * (r + gamma * max_q_sa_next - q_sa)
        self._qmatrix.set_value(action, state, new_q_sa)