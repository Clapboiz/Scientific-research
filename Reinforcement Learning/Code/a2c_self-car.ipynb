{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def OurModel(input_shape, action_space, lr):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\", input_shape=input_shape)(X_input)\n",
    "    #X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    #X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten(input_shape=input_shape)(X_input)\n",
    "\n",
    "    X = Dense(512, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(256, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(64, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    action = Dense(action_space, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "    value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    Actor = Model(inputs = X_input, outputs = action)\n",
    "    Actor.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    Critic = Model(inputs = X_input, outputs = value)\n",
    "    Critic.compile(loss='mse', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    return Actor, Critic\n",
    "\n",
    "class A2CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.max_average = 5, -21.0 # specific for pong\n",
    "        self.lr = 0.000025\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate games and plot memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A2C_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.Actor, self.Critic = OurModel(input_shape=self.state_size, action_space = self.action_size, lr=self.lr)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward):\n",
    "        # store episode actions to memory\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "                \n",
    "    def replay(self):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # Get Critic network predictions\n",
    "        values = self.Critic.predict(states)[:, 0]\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "        # training Actor and Critic networks\n",
    "        self.Actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "        self.Critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def load(self, Actor_name, Critic_name):\n",
    "        self.Actor = load_model(Actor_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.save(self.Model_name + '_Actor.h5')\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.Model_name+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)     \n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.remember(state, action, reward)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    # saving best models\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save()\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    self.replay()\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment Pong doesn't exist. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\Desktop\\Scientific-research\\Reinforcement Learning\\Code\\a2c_self-car.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPong-v0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     agent \u001b[39m=\u001b[39m A2CAgent(env_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     agent\u001b[39m.\u001b[39mrun()\n",
      "\u001b[1;32md:\\Users\\Desktop\\Scientific-research\\Reinforcement Learning\\Code\\a2c_self-car.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env_name):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Initialization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Environment and PPO parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_name \u001b[39m=\u001b[39m env_name       \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Desktop/Scientific-research/Reinforcement%20Learning/Code/a2c_self-car.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEPISODES, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_average \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m21.0\u001b[39m \u001b[39m# specific for pong\u001b[39;00m\n",
      "File \u001b[1;32md:\\Set up app\\Python\\lib\\site-packages\\gym\\envs\\registration.py:569\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    564\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the latest versioned environment `\u001b[39m\u001b[39m{\u001b[39;00mnew_env_id\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead of the unversioned environment `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m         )\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m spec_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 569\u001b[0m         _check_version_exists(ns, name, version)\n\u001b[0;32m    570\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo registered env with id: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m _kwargs \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\Set up app\\Python\\lib\\site-packages\\gym\\envs\\registration.py:219\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[1;34m(ns, name, version)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m get_env_id(ns, name, version) \u001b[39min\u001b[39;00m registry:\n\u001b[0;32m    217\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m _check_name_exists(ns, name)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\Set up app\\Python\\lib\\site-packages\\gym\\envs\\registration.py:197\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[1;34m(ns, name)\u001b[0m\n\u001b[0;32m    194\u001b[0m namespace_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in namespace \u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m ns \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m suggestion_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid you mean: `\u001b[39m\u001b[39m{\u001b[39;00msuggestion[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m`?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m suggestion \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 197\u001b[0m \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mNameNotFound(\n\u001b[0;32m    198\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnvironment \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist\u001b[39m\u001b[39m{\u001b[39;00mnamespace_msg\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00msuggestion_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m )\n",
      "\u001b[1;31mNameNotFound\u001b[0m: Environment Pong doesn't exist. "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    env_name = 'Pong-v0'\n",
    "\n",
    "    agent = A2CAgent(env_name)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
